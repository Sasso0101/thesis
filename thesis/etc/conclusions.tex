\chapter{Conclusions}
\label{cha:conclusions}
The efficient execution of Breadth-First Search, a foundational algorithm in graph analytics, is a significant challenge on modern multicore architectures. The primary performance bottleneck is not computational complexity but rather the irregular memory access patterns inherent in traversing large-scale graphs, which leads to poor cache utilization. This thesis addressed this challenge by investigating, implementing, and evaluating two distinct parallel optimization strategies specifically tailored for large-diameter graphs, where the Top-Down traversal approach is most effective. The central hypothesis was that a holistic optimization approach, which considers both the data structure's memory layout and the parallel execution model, is necessary to achieve high performance and scalability.

This work presented two primary contributions. The first was a cache-optimized BFS implementation in C++ using OpenMP, built upon a novel MergedCSR data structure. This format improves spatial locality by co-locating a vertex's metadata directly with its adjacency list. The second contribution was a case study in explicit parallelization, an implementation written in C using the pthreads library. This version provided fine-grained control over the parallel execution model, featuring a persistent thread pool, a dynamic work-stealing mechanism, and a lightweight, custom sense-reversal barrier.

The performance evaluation, conducted across a diverse range of hardware platforms and real-world datasets, yielded several key findings. Firstly, the MergedCSR data structure proved highly effective for purely Top-Down workloads. The OpenMP implementation using this structure achieved a geometric mean speedup of up to 1.5x over the GAP Benchmark Suite baseline on road networks and random geometric graphs, validating the hypothesis that aligning the data structure with the memory hierarchy is critical in performance optimization.

Secondly, the comparative analysis of parallelization strategies revealed the limitations of high-level frameworks for this specific problem domain. While the OpenMP implementation was effective at low core counts, it exhibited negative scaling at higher thread counts due to the overhead of its general-purpose synchronization primitives. In contrast, the explicit pthreads implementation demonstrated superior scalability, achieving a geometric mean speedup of up to 1.55x over the OpenMP version. This finding underscores the importance of a custom parallel execution model for algorithms with fine-grained synchronization. The study also highlighted the limitations of a specialized, Top-Down-only approach, as both implementations were outperformed by the direction-optimizing gapbs on datasets with small-diameter characteristics.

Finally, the evaluation on diverse hardware platforms highlighted the importance of architectural awareness. While the pthreads implementation scaled robustly on the x86-based AMD platform and showed excellent scaling on the RISC-V-based Pioneer board, it suffered from a sharp performance degradation at high core counts on the ARM-based NVIDIA Grace CPU.

Several directions for future research emerge from this work. The most immediate is the extension of these implementations to support a hybrid, direction-optimizing strategy. This would involve adapting the MergedCSR format to support the Bottom-Up traversal's bitmap-based lookups. Another promising direction is the application of these principles, namely, cache-aware data layouts and scalable custom synchronization, to other memory-bound graph algorithms, such as Single-Source Shortest Path or PageRank.

Beyond algorithmic extensions, the OpenMP implementation and the pthreads implementation could serve as a benchmark for evaluating the overhead and scalability of the two parallelization models. Furthermore, for the pthreads implementation, the synchronization mechanisms themselves can be implemented with different primitives, such as atomics or NUMA-aware locks. This modularity allows the implementation to serve as a benchmark for evaluating the performance of these synchronization constructs on different architectures. Also, the existing sense-reversal barrier could be replaced with other mechanisms, such as tree-based or tournament barriers, to analyze how different synchronization strategies interact with a given platform's cache coherence protocol. As demonstrated by the performance difference on the NVIDIA Grace CPU, this approach can reveal performance bottlenecks in both hardware architectures and compiler implementations.

Finally, a more in-depth investigation into the cache behavior using low-level performance counters could further determine the reasons for the observed performance differences. Such an analysis, combined with a direct evaluation of the energy consumption of these implementations, would provide valuable insights, explicitly linking reduced runtimes and fewer memory stalls to improvements in energy efficiency on modern processors.